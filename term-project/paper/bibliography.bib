@article{Thapa2020,
  author   = {Thapa, Ranjita and Zhang, Kai and Snavely, Noah and Belongie, Serge and Khan, Awais},
  title    = {The Plant Pathology Challenge 2020 data set to classify foliar disease of apples},
  journal  = {Applications in Plant Sciences},
  volume   = {8},
  number   = {9},
  pages    = {e11390},
  keywords = {apple orchards, computer vision, convolutional neural network, disease classification, machine learning},
  doi      = {https://doi.org/10.1002/aps3.11390},
  url      = {https://bsapubs.onlinelibrary.wiley.com/doi/abs/10.1002/aps3.11390},
  eprint   = {https://bsapubs.onlinelibrary.wiley.com/doi/pdf/10.1002/aps3.11390},
  year     = {2020}
}

@inproceedings{ResNet2016,
  author    = {Kaiming He and
               Xiangyu Zhang and
               Shaoqing Ren and
               Jian Sun},
  title     = {Deep Residual Learning for Image Recognition},
  booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2016, Las Vegas, NV, USA, June 27-30, 2016},
  pages     = {770--778},
  publisher = {{IEEE} Computer Society},
  year      = {2016},
  url       = {https://doi.org/10.1109/CVPR.2016.90},
  doi       = {10.1109/CVPR.2016.90},
  timestamp = {Wed, 16 Oct 2019 14:14:50 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/HeZRS16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{InceptionNetV1,
  author    = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  biburl    = {https://www.bibsonomy.org/bibtex/2b68c97c21a0309ebfac7ee3e60fe099f/msteininger},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  keywords  = {imported},
  pages     = {1--9},
  title     = {Going deeper with convolutions},
  year      = 2015
}

@inproceedings{InceptionNetV3,
  author    = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Rethinking the Inception Architecture for Computer Vision},
  year      = {2016},
  volume    = {},
  number    = {},
  pages     = {2818-2826},
  doi       = {10.1109/CVPR.2016.308}
}

@inproceedings{InceptionNetV4,
  author    = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A.},
  title     = {Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning},
  year      = {2017},
  publisher = {AAAI Press},
  abstract  = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question: Are there any benefits to combining Inception architectures with residual connections? Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4 networks, we achieve 3.08% top-5 error on the test set of the ImageNet classification (CLS) challenge.},
  booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
  pages     = {4278--4284},
  numpages  = {7},
  location  = {San Francisco, California, USA},
  series    = {AAAI'17}
}

@misc{MobileNet2017,
  added-at    = {2020-05-11T23:24:49.000+0200},
  author      = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  biburl      = {https://www.bibsonomy.org/bibtex/2c5c68e31f5b5dea865fb7feacc5757c2/sohnki},
  description = {[1704.04861] MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications},
  interhash   = {962bc072d9243f18fac8a2ef7663970b},
  intrahash   = {c5c68e31f5b5dea865fb7feacc5757c2},
  keywords    = {MobileNet order1},
  note        = {cite arxiv:1704.04861},
  timestamp   = {2020-06-02T20:03:57.000+0200},
  title       = {MobileNets: Efficient Convolutional Neural Networks for Mobile Vision
                 Applications},
  url         = {http://arxiv.org/abs/1704.04861},
  year        = 2017
}

@article{ImageNet2014,
  author     = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
  title      = {ImageNet Large Scale Visual Recognition Challenge},
  year       = {2015},
  issue_date = {December  2015},
  publisher  = {Kluwer Academic Publishers},
  address    = {USA},
  volume     = {115},
  number     = {3},
  issn       = {0920-5691},
  url        = {https://doi.org/10.1007/s11263-015-0816-y},
  doi        = {10.1007/s11263-015-0816-y},
  abstract   = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
  journal    = {Int. J. Comput. Vision},
  month      = {dec},
  pages      = {211-252},
  numpages   = {42},
  keywords   = {Object detection, Benchmark, Large-scale, Dataset, Object recognition}
}

@article{CIFAR,
  author    = {Krizhevsky, Alex},
  journal   = {Technical Report},
  keywords  = {},
  pages     = {32--33},
  timestamp = {2021-01-21T03:01:11.000+0100},
  title     = {Learning Multiple Layers of Features from Tiny Images},
  url       = {https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf},
  year      = 2009
}

@inproceedings{FocalLoss,
  author    = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollar, Piotr},
  title     = {Focal Loss for Dense Object Detection},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  month     = {Oct},
  year      = {2017}
}

@inproceedings{EfficientNet,
  title     = {{E}fficient{N}et: Rethinking Model Scaling for Convolutional Neural Networks},
  author    = {Tan, Mingxing and Le, Quoc},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  pages     = {6105--6114},
  year      = {2019},
  editor    = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume    = {97},
  series    = {Proceedings of Machine Learning Research},
  month     = {09--15 Jun},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v97/tan19a/tan19a.pdf},
  url       = {https://proceedings.mlr.press/v97/tan19a.html},
  abstract  = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are given. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves stateof-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet (Huang et al., 2018). Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flower (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.}
}

@inproceedings{BagOfTricks,
  author    = {He, Tong and Zhang, Zhi and Zhang, Hang and Zhang, Zhongyue and Xie, Junyuan and Li, Mu},
  booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Bag of Tricks for Image Classification with Convolutional Neural Networks},
  year      = {2019},
  volume    = {},
  number    = {},
  pages     = {558--567},
  doi       = {10.1109/CVPR.2019.00065}
}

@article{fastai,
  author         = {Howard, Jeremy and Gugger, Sylvain},
  title          = {Fastai: A Layered API for Deep Learning},
  journal        = {Information},
  volume         = {11},
  year           = {2020},
  number         = {2},
  article-number = {108},
  url            = {https://www.mdpi.com/2078-2489/11/2/108},
  issn           = {2078-2489},
  abstract       = {fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. It aims to do both things without substantial compromises in ease of use, flexibility, or performance. This is possible thanks to a carefully layered architecture, which expresses common underlying patterns of many deep learning and data processing techniques in terms of decoupled abstractions. These abstractions can be expressed concisely and clearly by leveraging the dynamism of the underlying Python language and the flexibility of the PyTorch library. fastai includes: a new type dispatch system for Python along with a semantic type hierarchy for tensors; a GPU-optimized computer vision library which can be extended in pure Python; an optimizer which refactors out the common functionality of modern optimizers into two basic pieces, allowing optimization algorithms to be implemented in 4&ndash;5 lines of code; a novel 2-way callback system that can access any part of the data, model, or optimizer and change it at any point during training; a new data block API; and much more. We used this library to successfully create a complete deep learning course, which we were able to write more quickly than using previous approaches, and the code was more clear. The library is already in wide use in research, industry, and teaching.},
  doi            = {10.3390/info11020108}
}

@misc{SmithSuperConvergence,
  title  = {Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates},
  author = {Leslie N. Smith and Nicholay Topin},
  year   = {2018},
  url    = {https://openreview.net/forum?id=H1A5ztj3b}
}

@article{Smith2015,
  author     = {Leslie N. Smith},
  title      = {No More Pesky Learning Rate Guessing Games},
  journal    = {CoRR},
  volume     = {abs/1506.01186},
  year       = {2015},
  url        = {http://arxiv.org/abs/1506.01186},
  eprinttype = {arXiv},
  eprint     = {1506.01186},
  timestamp  = {Mon, 13 Aug 2018 16:47:53 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/Smith15a.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{Smith2018,
  author     = {Leslie N. Smith},
  title      = {A disciplined approach to neural network hyper-parameters: Part 1
                - learning rate, batch size, momentum, and weight decay},
  journal    = {CoRR},
  volume     = {abs/1803.09820},
  year       = {2018},
  url        = {http://arxiv.org/abs/1803.09820},
  eprinttype = {arXiv},
  eprint     = {1803.09820},
  timestamp  = {Mon, 13 Aug 2018 16:46:45 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1803-09820.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{Shorten2019,
  author     = {Shorten, Connor and Khoshgoftaar, Taghi M.},
  da         = {2019/07/06},
  doi        = {10.1186/s40537-019-0197-0},
  id         = {Shorten2019},
  isbn       = {2196-1115},
  journal    = {Journal of Big Data},
  number     = {1},
  pages      = {60},
  title      = {A survey on Image Data Augmentation for Deep Learning},
  ty         = {JOUR},
  url        = {https://doi.org/10.1186/s40537-019-0197-0},
  volume     = {6},
  year       = {2019},
  bdsk-url-1 = {https://doi.org/10.1186/s40537-019-0197-0}
}

@inproceedings{sklearn_api,
  author    = {Lars Buitinck and Gilles Louppe and Mathieu Blondel and
               Fabian Pedregosa and Andreas Mueller and Olivier Grisel and
               Vlad Niculae and Peter Prettenhofer and Alexandre Gramfort
               and Jaques Grobler and Robert Layton and Jake VanderPlas and
               Arnaud Joly and Brian Holt and Ga{\"{e}}l Varoquaux},
  title     = {{API} design for machine learning software: experiences from the scikit-learn
               project},
  booktitle = {ECML PKDD Workshop: Languages for Data Mining and Machine Learning},
  year      = {2013},
  pages     = {108--122}
}